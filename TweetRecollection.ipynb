{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8caeaa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from twitter_authentication import bearer_token\n",
    "import time\n",
    "import pandas as pd\n",
    "import json \n",
    "import pickle\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import glob, os, json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c1fdfe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_load(filepath):\n",
    "    with open(filepath, \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "\n",
    "def json_dump(data, filepath, pretty_format = True):\n",
    "    with open(filepath, 'w') as fw:\n",
    "        if pretty_format:\n",
    "            json.dump(data, fw, indent=2, sort_keys=True)\n",
    "        else:\n",
    "            json.dump(data, fw)\n",
    "\n",
    "def pickle_dump(obj, pickle_filepath):\n",
    "    with open(pickle_filepath, \"wb\") as f:\n",
    "        pickle.dump(obj, f, protocol=2)\n",
    "\n",
    "def pickle_load(pickle_filepath):\n",
    "    with open(pickle_filepath, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8e0480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0dd16a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(place_dict = {},\n",
    "               user_dict = {},\n",
    "               tweet_dict = {},\n",
    "                next_token = None,\n",
    "               alz_tweets = [], \n",
    "                query = 'alzheimers disease -is:retweet place_country:US',\n",
    "              user_fields = ['username', 'public_metrics', 'description', 'location'],\n",
    "              place_fields = ['place_type', 'geo'],\n",
    "              tweet_fields = ['created_at', 'geo', 'public_metrics', 'text'],\n",
    "              expansions = ['author_id', 'geo.place_id'],\n",
    "              start_time = '2010-01-01T00:00:00Z',\n",
    "               end_time = '2023-01-15T00:00:00Z'\n",
    "              ):\n",
    "    place_keys= [\"full_name\", 'id', 'contained_within', 'country', 'country_code', 'geo', 'name', 'place_type' ]\n",
    "    user_keys= ['id', 'name', 'username', 'created_at', \n",
    "            'description', \"entities\", 'location', 'pinned_tweet_id', \n",
    "            'profile_image_url', 'protected', 'public_metrics', 'url', 'verified', 'withheld']\n",
    "    tweet_keys = ['id', 'text', 'author_id', 'context_annotations',\n",
    "             'conversation_id', 'entities', 'in_reply_to_user_id', 'lang',\n",
    "             'non_public_metrics', 'organic_metrics', 'possibly_sensitive',\n",
    "             'promoted_metrics', 'public_metrics', 'referenced_tweets', 'reply_settings',\n",
    "             'source', 'withheld']    \n",
    "\n",
    "\n",
    "    for tweet in tweepy.Paginator(client.search_all_tweets, \n",
    "                                     query = query,\n",
    "                                     user_fields = user_fields,\n",
    "                                     place_fields = place_fields,\n",
    "                                     tweet_fields = tweet_fields,\n",
    "                                     expansions = expansions,\n",
    "                                     start_time = start_time,\n",
    "                                     end_time = end_time,\n",
    "                                  pagination_token = next_token,\n",
    "                                  max_results=500):\n",
    "        time.sleep(1)\n",
    "        alz_tweets.append(tweet)\n",
    "    \n",
    "    \n",
    "    # Loop through each response object\n",
    "    for response in alz_tweets:\n",
    "        # Take all of the users, and put them into a dictionary of dictionaries with the info we want to keep\n",
    "        for place in response.includes['places']:\n",
    "            place_obj = {}\n",
    "            for key in place_keys:\n",
    "                place_obj[key] = place[key]\n",
    "            place_dict[place_obj['id']] = place_obj\n",
    "        for user in response.includes['users']:\n",
    "            user_obj = {}\n",
    "            for key in user_keys:\n",
    "                user_obj[key] = user[key]\n",
    "            user_dict[user_obj['id']] = user_obj\n",
    "\n",
    "        for tweet in response.data:\n",
    "            tweet_obj = {}\n",
    "            for key in tweet_keys:\n",
    "                tweet_obj[key] = getattr(tweet, key)\n",
    "            tweet_dict[tweet_obj['id']] = tweet_obj\n",
    "    \n",
    "    return place_dict, user_dict, tweet_dict, next_token, alz_tweets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "60c9f8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token = None\n",
    "place_dict = {}\n",
    "user_dict = {}\n",
    "tweet_dict = {}\n",
    "alz_tweets = []\n",
    "\n",
    "\n",
    "\n",
    "place_dict, user_dict, tweet_dict, next_token, alz_tweets = get_tweets(place_dict,user_dict,tweet_dict, next_token, alz_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c8eb9dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = alz_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4835e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(response.data))\n",
    "# for tweet in response.data:\n",
    "#     print(dict(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2b638ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data = []\n",
    "for tweet in response.data:\n",
    "    #print(dict(tweet))\n",
    "    for place in response.includes['places']:\n",
    "        if tweet.geo is not None:\n",
    "            if tweet.geo['place_id'] == place['id']:\n",
    "                tweet_info = {\n",
    "                    \"content\": tweet.text,\n",
    "                    \"geo_id\": place['id'] if place is not None else None,\n",
    "                    \"geo_info\": dict(place)\n",
    "                }\n",
    "                tweets_data.append(tweet_info)\n",
    "                #print(dict(place))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3c1d59fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303\n"
     ]
    }
   ],
   "source": [
    "print(len(tweets_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d6c3278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tweet in tweets_data:\n",
    "#     print(tweet)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4d2f5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dump(tweets_data, filepath = \"Full_List/alzheimers_disease.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cb672a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets2(place_dict = {},\n",
    "               user_dict = {},\n",
    "               tweet_dict = {},\n",
    "                next_token = None,\n",
    "               alz_tweets = [],\n",
    "                query = 'alzheimer\\'s disease -is:retweet place_country:US',\n",
    "              user_fields = ['username', 'public_metrics', 'description', 'location'],\n",
    "              place_fields = ['place_type', 'geo'],\n",
    "              tweet_fields = ['created_at', 'geo', 'public_metrics', 'text'],\n",
    "              expansions = ['author_id', 'geo.place_id'],\n",
    "              start_time = '2010-01-01T00:00:00Z',\n",
    "               end_time = '2023-01-15T00:00:00Z'\n",
    "              ):\n",
    "    place_keys= [\"full_name\", 'id', 'contained_within', 'country', 'country_code', 'geo', 'name', 'place_type' ]\n",
    "    user_keys= ['id', 'name', 'username', 'created_at', \n",
    "            'description', \"entities\", 'location', 'pinned_tweet_id', \n",
    "            'profile_image_url', 'protected', 'public_metrics', 'url', 'verified', 'withheld']\n",
    "    tweet_keys = ['id', 'text', 'author_id', 'context_annotations',\n",
    "             'conversation_id', 'entities', 'in_reply_to_user_id', 'lang',\n",
    "             'non_public_metrics', 'organic_metrics', 'possibly_sensitive',\n",
    "             'promoted_metrics', 'public_metrics', 'referenced_tweets', 'reply_settings',\n",
    "             'source', 'withheld']    \n",
    "\n",
    "\n",
    "    for tweet in tweepy.Paginator(client.search_all_tweets, \n",
    "                                     query = query,\n",
    "                                     user_fields = user_fields,\n",
    "                                     place_fields = place_fields,\n",
    "                                     tweet_fields = tweet_fields,\n",
    "                                     expansions = expansions,\n",
    "                                     start_time = start_time,\n",
    "                                     end_time = end_time,\n",
    "                                  pagination_token = next_token,\n",
    "                                  max_results=500):\n",
    "        time.sleep(1)\n",
    "        alz_tweets.append(tweet)\n",
    "    \n",
    "    \n",
    "    return alz_tweets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c9a882cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token = None\n",
    "place_dict = {}\n",
    "user_dict = {}\n",
    "tweet_dict = {}\n",
    "alz_tweets = []\n",
    "\n",
    "\n",
    "\n",
    "alz_tweets = get_tweets2(place_dict,user_dict,tweet_dict, next_token, alz_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "782e19f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = alz_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "496853f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data = []\n",
    "for tweet in response.data:\n",
    "    #print(dict(tweet))\n",
    "    for place in response.includes['places']:\n",
    "        if tweet.geo is not None:\n",
    "            if tweet.geo['place_id'] == place['id']:\n",
    "                tweet_info = {\n",
    "                    \"content\": tweet.text,\n",
    "                    \"geo_id\": place['id'] if place is not None else None,\n",
    "                    \"geo_info\": dict(place)\n",
    "                }\n",
    "                tweets_data.append(tweet_info)\n",
    "                #print(dict(place))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "891e4af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dump(tweets_data, filepath = \"Full_List/alzheimer\\'s_disease.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c2dc2a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets3(place_dict = {},\n",
    "               user_dict = {},\n",
    "               tweet_dict = {},\n",
    "                next_token = None,\n",
    "               alz_tweets = [],\n",
    "                query = 'alzheimer -is:retweet place_country:US',\n",
    "              user_fields = ['username', 'public_metrics', 'description', 'location'],\n",
    "              place_fields = ['place_type', 'geo'],\n",
    "              tweet_fields = ['created_at', 'geo', 'public_metrics', 'text'],\n",
    "              expansions = ['author_id', 'geo.place_id'],\n",
    "              start_time = '2010-01-01T00:00:00Z',\n",
    "               end_time = '2023-01-15T00:00:00Z'\n",
    "              ):\n",
    "    place_keys= [\"full_name\", 'id', 'contained_within', 'country', 'country_code', 'geo', 'name', 'place_type' ]\n",
    "    user_keys= ['id', 'name', 'username', 'created_at', \n",
    "            'description', \"entities\", 'location', 'pinned_tweet_id', \n",
    "            'profile_image_url', 'protected', 'public_metrics', 'url', 'verified', 'withheld']\n",
    "    tweet_keys = ['id', 'text', 'author_id', 'context_annotations',\n",
    "             'conversation_id', 'entities', 'in_reply_to_user_id', 'lang',\n",
    "             'non_public_metrics', 'organic_metrics', 'possibly_sensitive',\n",
    "             'promoted_metrics', 'public_metrics', 'referenced_tweets', 'reply_settings',\n",
    "             'source', 'withheld']    \n",
    "\n",
    "\n",
    "    for tweet in tweepy.Paginator(client.search_all_tweets, \n",
    "                                     query = query,\n",
    "                                     user_fields = user_fields,\n",
    "                                     place_fields = place_fields,\n",
    "                                     tweet_fields = tweet_fields,\n",
    "                                     expansions = expansions,\n",
    "                                     start_time = start_time,\n",
    "                                     end_time = end_time,\n",
    "                                  pagination_token = next_token,\n",
    "                                  max_results=500):\n",
    "        time.sleep(1)\n",
    "        alz_tweets.append(tweet)\n",
    "    \n",
    "    \n",
    "    return alz_tweets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a152b7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 75 seconds.\n"
     ]
    }
   ],
   "source": [
    "next_token = None\n",
    "place_dict = {}\n",
    "user_dict = {}\n",
    "tweet_dict = {}\n",
    "alz_tweets = []\n",
    "\n",
    "\n",
    "\n",
    "alz_tweets = get_tweets3(place_dict,user_dict,tweet_dict, next_token, alz_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "851b8e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = alz_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2b51d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data = []\n",
    "for tweet in response.data:\n",
    "    #print(dict(tweet))\n",
    "    for place in response.includes['places']:\n",
    "        if tweet.geo is not None:\n",
    "            if tweet.geo['place_id'] == place['id']:\n",
    "                tweet_info = {\n",
    "                    \"content\": tweet.text,\n",
    "                    \"geo_id\": place['id'] if place is not None else None,\n",
    "                    \"geo_info\": dict(place)\n",
    "                }\n",
    "                tweets_data.append(tweet_info)\n",
    "                #print(dict(place))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "97414732",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dump(tweets_data, filepath = \"Full_List/alzheimer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5af620f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets3(place_dict = {},\n",
    "               user_dict = {},\n",
    "               tweet_dict = {},\n",
    "                next_token = None,\n",
    "               alz_tweets = [],\n",
    "                query = 'alzheimers -is:retweet place_country:US',\n",
    "              user_fields = ['username', 'public_metrics', 'description', 'location'],\n",
    "              place_fields = ['place_type', 'geo'],\n",
    "              tweet_fields = ['created_at', 'geo', 'public_metrics', 'text'],\n",
    "              expansions = ['author_id', 'geo.place_id'],\n",
    "              start_time = '2010-01-01T00:00:00Z',\n",
    "               end_time = '2023-01-15T00:00:00Z'\n",
    "              ):\n",
    "    place_keys= [\"full_name\", 'id', 'contained_within', 'country', 'country_code', 'geo', 'name', 'place_type' ]\n",
    "    user_keys= ['id', 'name', 'username', 'created_at', \n",
    "            'description', \"entities\", 'location', 'pinned_tweet_id', \n",
    "            'profile_image_url', 'protected', 'public_metrics', 'url', 'verified', 'withheld']\n",
    "    tweet_keys = ['id', 'text', 'author_id', 'context_annotations',\n",
    "             'conversation_id', 'entities', 'in_reply_to_user_id', 'lang',\n",
    "             'non_public_metrics', 'organic_metrics', 'possibly_sensitive',\n",
    "             'promoted_metrics', 'public_metrics', 'referenced_tweets', 'reply_settings',\n",
    "             'source', 'withheld']    \n",
    "\n",
    "\n",
    "    for tweet in tweepy.Paginator(client.search_all_tweets, \n",
    "                                     query = query,\n",
    "                                     user_fields = user_fields,\n",
    "                                     place_fields = place_fields,\n",
    "                                     tweet_fields = tweet_fields,\n",
    "                                     expansions = expansions,\n",
    "                                     start_time = start_time,\n",
    "                                     end_time = end_time,\n",
    "                                  pagination_token = next_token,\n",
    "                                  max_results=500):\n",
    "        time.sleep(1)\n",
    "        alz_tweets.append(tweet)\n",
    "    \n",
    "    \n",
    "    return alz_tweets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ceac135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token = None\n",
    "place_dict = {}\n",
    "user_dict = {}\n",
    "tweet_dict = {}\n",
    "alz_tweets = []\n",
    "\n",
    "\n",
    "\n",
    "alz_tweets = get_tweets2(place_dict,user_dict,tweet_dict, next_token, alz_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ec80aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = alz_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f2baa30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data = []\n",
    "for tweet in response.data:\n",
    "    #print(dict(tweet))\n",
    "    for place in response.includes['places']:\n",
    "        if tweet.geo is not None:\n",
    "            if tweet.geo['place_id'] == place['id']:\n",
    "                tweet_info = {\n",
    "                    \"content\": tweet.text,\n",
    "                    \"geo_id\": place['id'] if place is not None else None,\n",
    "                    \"geo_info\": dict(place)\n",
    "                }\n",
    "                tweets_data.append(tweet_info)\n",
    "                #print(dict(place))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f9f24cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dump(tweets_data, filepath = \"Full_List/alzheimers.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
