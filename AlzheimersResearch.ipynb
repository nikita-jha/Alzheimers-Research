{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c79524b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from twitter_authentication import bearer_token\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3747426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_load(filepath):\n",
    "    with open(filepath, \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "\n",
    "def json_dump(data, filepath, pretty_format = True):\n",
    "    with open(filepath, 'w') as fw:\n",
    "        if pretty_format:\n",
    "            json.dump(data, fw, indent=2, sort_keys=True)\n",
    "        else:\n",
    "            json.dump(data, fw)\n",
    "\n",
    "def pickle_dump(obj, pickle_filepath):\n",
    "    with open(pickle_filepath, \"wb\") as f:\n",
    "        pickle.dump(obj, f, protocol=2)\n",
    "\n",
    "def pickle_load(pickle_filepath):\n",
    "    with open(pickle_filepath, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6a20626",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73676d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token = None\n",
    "place_dict = {}\n",
    "user_dict = {}\n",
    "tweet_dict = {}\n",
    "\n",
    "\n",
    "\n",
    "place_dict, user_dict, tweet_dict, next_token = get_tweets(place_dict,user_dict,tweet_dict, next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "92b7954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "while next_token is not None:\n",
    "    place_dict, user_dict, tweet_dict, next_token = get_tweets(place_dict,user_dict,tweet_dict, next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "163250cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(place_dict = {},\n",
    "               user_dict = {},\n",
    "               tweet_dict = {},\n",
    "                next_token = None,\n",
    "               query = 'alzheimers disease OR alzheimer -is:retweet place_country:US',\n",
    "              user_fields = ['username', 'public_metrics', 'description', 'location'],\n",
    "              place_fields = ['place_type', 'geo'],\n",
    "              tweet_fields = ['created_at', 'geo', 'public_metrics', 'text'],\n",
    "              expansions = ['author_id', 'geo.place_id'],\n",
    "              start_time = '2018-01-01T00:00:00Z',\n",
    "               end_time = '2020-01-15T00:00:00Z'\n",
    "              ):\n",
    "    place_keys= [\"full_name\", 'id', 'contained_within', 'country', 'country_code', 'geo', 'name', 'place_type' ]\n",
    "    user_keys= ['id', 'name', 'username', 'created_at', \n",
    "            'description', \"entities\", 'location', 'pinned_tweet_id', \n",
    "            'profile_image_url', 'protected', 'public_metrics', 'url', 'verified', 'withheld']\n",
    "    tweet_keys = ['id', 'text', 'author_id', 'context_annotations',\n",
    "             'conversation_id', 'entities', 'in_reply_to_user_id', 'lang',\n",
    "             'non_public_metrics', 'organic_metrics', 'possibly_sensitive',\n",
    "             'promoted_metrics', 'public_metrics', 'referenced_tweets', 'reply_settings',\n",
    "             'source', 'withheld']    \n",
    "\n",
    "\n",
    "    for tweet in tweepy.Paginator(client.search_all_tweets, \n",
    "                                     query = query,\n",
    "                                     user_fields = user_fields,\n",
    "                                     place_fields = place_fields,\n",
    "                                     tweet_fields = tweet_fields,\n",
    "                                     expansions = expansions,\n",
    "                                     start_time = start_time,\n",
    "                                     end_time = end_time,\n",
    "                                  pagination_token = next_token,\n",
    "                                  max_results=500):\n",
    "        time.sleep(1)\n",
    "        alz_tweets.append(tweet)\n",
    "    \n",
    "    \n",
    "    # Loop through each response object\n",
    "    for response in alz_tweets:\n",
    "        # Take all of the users, and put them into a dictionary of dictionaries with the info we want to keep\n",
    "        for place in response.includes['places']:\n",
    "            place_obj = {}\n",
    "            for key in place_keys:\n",
    "                place_obj[key] = place[key]\n",
    "            place_dict[place_obj['id']] = place_obj\n",
    "        for user in response.includes['users']:\n",
    "            user_obj = {}\n",
    "            for key in user_keys:\n",
    "                user_obj[key] = user[key]\n",
    "            user_dict[user_obj['id']] = user_obj\n",
    "\n",
    "        for tweet in response.data:\n",
    "            tweet_obj = {}\n",
    "            for key in tweet_keys:\n",
    "                tweet_obj[key] = getattr(tweet, key)\n",
    "            tweet_dict[tweet_obj['id']] = tweet_obj\n",
    "#     if len(alz_tweets) == 0:\n",
    "#         next_token = None\n",
    "#     else:\n",
    "#         if 'next_token' is in alz_tweets[0].meta:\n",
    "#             next_token = alz_tweets[0].meta['next_token']\n",
    "    \n",
    "    return place_dict, user_dict, tweet_dict, next_token\n",
    "\n",
    "    print(len(alz_tweets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9e7816e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('b26v89c19zqg8o3fnlo0yxc7761hth3gu5oeu7ti2ma2l',\n",
       " 'b26v89c19zqg8o3fn0dor7lfae0eh6204lplge3rqkv3x')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alz_tweets[0].meta['next_token'], alz_tweets[1].meta['next_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c5b3fc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 497\n",
      "1 491\n",
      "2 342\n"
     ]
    }
   ],
   "source": [
    "for idx, response in enumerate(alz_tweets):\n",
    "    print(idx, len(response.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3001dca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(alz_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ef83e4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'newest_id': '1017433152736804870',\n",
       " 'oldest_id': '947865213335998464',\n",
       " 'result_count': 339}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alz_tweets[-1].meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce94f8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['users', 'places'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = alz_tweets[0]\n",
    "response.includes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "514cc112",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dump(place_dict, filepath = \"place.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a6453421",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dump(user_dict, filepath = \"user.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0a58017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dump(tweet_dict, filepath = \"tweet.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6a5f21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Place Attributes\n",
    "place_keys= [\"full_name\", 'id', 'contained_within', 'country', 'country_code', 'geo', 'name', 'place_type' ]\n",
    "place_dict = {}\n",
    "#User Attributes\n",
    "user_keys= ['id', 'name', 'username', 'created_at', \n",
    "            'description', \"entities\", 'location', 'pinned_tweet_id', \n",
    "            'profile_image_url', 'protected', 'public_metrics', 'url', 'verified', 'withheld']\n",
    "user_dict = {}\n",
    "#Tweet Attributes\n",
    "tweet_keys = ['id', 'text', 'author_id', 'context_annotations',\n",
    "             'conversation_id', 'entities', 'in_reply_to_user_id', 'lang',\n",
    "             'non_public_metrics', 'organic_metrics', 'possibly_sensitive',\n",
    "             'promoted_metrics', 'public_metrics', 'referenced_tweets', 'reply_settings',\n",
    "             'source', 'withheld']\n",
    "tweet_dict = {}\n",
    "full_list=[]\n",
    "\n",
    "for response in alz_tweets: \n",
    "    for place in response.includes['places']:\n",
    "        place_obj = {}\n",
    "        for key in place_keys:\n",
    "            place_obj[key] = place[key]\n",
    "        place_dict[place_obj['id']] = place_obj\n",
    "    for user in response.includes['users']:\n",
    "        user_obj = {}\n",
    "        for key in user_keys:\n",
    "            user_obj[key] = user[key]\n",
    "        user_dict[user_obj['id']] = user_obj\n",
    "    for tweet in response.data:\n",
    "        tweet_obj = {}\n",
    "        for key in tweet_keys:\n",
    "            tweet_obj[key] = getattr(tweet, key)\n",
    "        tweet_dict[tweet_obj['id']] = tweet_obj\n",
    "    full_list.append(place_obj)\n",
    "    full_list.append(user_obj)\n",
    "    full_list.append(tweet_obj)\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d22b0eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'full_name': 'San Marcos, CA',\n",
       "  'id': 'a2c84129f9dcf69f',\n",
       "  'contained_within': [],\n",
       "  'country': None,\n",
       "  'country_code': None,\n",
       "  'geo': {'type': 'Feature',\n",
       "   'bbox': [-117.2301723, 33.0907611, -117.103461, 33.186722],\n",
       "   'properties': {}},\n",
       "  'name': None,\n",
       "  'place_type': 'city'},\n",
       " {'id': 857761126305730560,\n",
       "  'name': 'Amanda Brandeis',\n",
       "  'username': 'amanda_brandeis',\n",
       "  'created_at': None,\n",
       "  'description': 'Visual Storyteller + Emmy Award Winning Journalist',\n",
       "  'entities': None,\n",
       "  'location': 'San Diego, CA',\n",
       "  'pinned_tweet_id': None,\n",
       "  'profile_image_url': None,\n",
       "  'protected': None,\n",
       "  'public_metrics': {'followers_count': 1082,\n",
       "   'following_count': 530,\n",
       "   'tweet_count': 1005,\n",
       "   'listed_count': 21},\n",
       "  'url': None,\n",
       "  'verified': None,\n",
       "  'withheld': None},\n",
       " {'id': 981221671792123904,\n",
       "  'text': \"A hiker has officially set off from San Diego on a 2,650-mile hike. \\n\\nHe's raising money for a disease that robs millions of their memories. More info on how to follow his journey or donate: https://t.co/TTl9PJU8gV @10News @alzassociation #Alzheimers https://t.co/wGkc7OUiPK\",\n",
       "  'author_id': 857761126305730560,\n",
       "  'context_annotations': [],\n",
       "  'conversation_id': None,\n",
       "  'entities': None,\n",
       "  'in_reply_to_user_id': None,\n",
       "  'lang': None,\n",
       "  'non_public_metrics': None,\n",
       "  'organic_metrics': None,\n",
       "  'possibly_sensitive': None,\n",
       "  'promoted_metrics': None,\n",
       "  'public_metrics': {'retweet_count': 3,\n",
       "   'reply_count': 1,\n",
       "   'like_count': 7,\n",
       "   'quote_count': 0,\n",
       "   'impression_count': 0},\n",
       "  'referenced_tweets': None,\n",
       "  'reply_settings': None,\n",
       "  'source': None,\n",
       "  'withheld': None},\n",
       " {'full_name': 'Vincennes, IN',\n",
       "  'id': 'bfadcb4bfcf83cef',\n",
       "  'contained_within': [],\n",
       "  'country': None,\n",
       "  'country_code': None,\n",
       "  'geo': {'type': 'Feature',\n",
       "   'bbox': [-87.547173, 38.625181, -87.433471, 38.736046],\n",
       "   'properties': {}},\n",
       "  'name': None,\n",
       "  'place_type': 'city'},\n",
       " {'id': 707969058499473408,\n",
       "  'name': 'EKP',\n",
       "  'username': 'The_Eric_Peters',\n",
       "  'created_at': None,\n",
       "  'description': 'RBV & Miller Stan account',\n",
       "  'entities': None,\n",
       "  'location': 'Vincennes, IN',\n",
       "  'pinned_tweet_id': None,\n",
       "  'profile_image_url': None,\n",
       "  'protected': None,\n",
       "  'public_metrics': {'followers_count': 243,\n",
       "   'following_count': 71,\n",
       "   'tweet_count': 1692,\n",
       "   'listed_count': 2},\n",
       "  'url': None,\n",
       "  'verified': None,\n",
       "  'withheld': None},\n",
       " {'id': 947865213335998464,\n",
       "  'text': 'Alzheimers disease is literally the worst thing on the planet',\n",
       "  'author_id': 707969058499473408,\n",
       "  'context_annotations': [],\n",
       "  'conversation_id': None,\n",
       "  'entities': None,\n",
       "  'in_reply_to_user_id': None,\n",
       "  'lang': None,\n",
       "  'non_public_metrics': None,\n",
       "  'organic_metrics': None,\n",
       "  'possibly_sensitive': None,\n",
       "  'promoted_metrics': None,\n",
       "  'public_metrics': {'retweet_count': 0,\n",
       "   'reply_count': 0,\n",
       "   'like_count': 12,\n",
       "   'quote_count': 0,\n",
       "   'impression_count': 0},\n",
       "  'referenced_tweets': None,\n",
       "  'reply_settings': None,\n",
       "  'source': None,\n",
       "  'withheld': None}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702e6031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
